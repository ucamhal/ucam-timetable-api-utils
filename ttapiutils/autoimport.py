"""
usage: ttapiutils autoimport [-X=<name>=<value>...] [options] <data-source> <domain> <path>...
       ttapiutils autoimport --complete-dry-run=<audit-dir>

In the first form, <data-source> provides new timetable XML data
to be imported into <path>s on <domain>, replacing existing data.

Data produced by <data-source> is taken to be the desired state
of <path>s on <domain> after the import, so details such as
using ttapiutils deletegen to calculate deletes needed to achieve
the new state are taken care of automatically.

In the second form, the yet-unimported data from a dir created
by --audit-trail with --dry-run can be imported. This is useful
for completing an import after manually verifying behaviour.

options:
    <data-source>
        The name of the data source to use to generate the import data,
        or "-" to read Timetable XML data on stdin.

    <domain>
        The domain name of the Timetable site to perform the XML import
        on.

    <path>...
        List of timetable paths to be targeted by the import. The data
        source MUST produce data for every path listed. Pre-existing
        event data in Timetable at <domain> will be replaced by the
        data generated by the <data-source>.

    --audit-trail=<base-dir>
        Create a timestamped subdirectory of <dir> containing a record
        of the data received, generated and sent by an invocation of
        this program.

    --user=<user>
        The username to authenticate with.

    --pass-envar=<envar>
        The name of the environment variable which contains the user's
        password. If none is provided the password will be prompted
        for on stdin.

    --https
    --no-https
        Use or don't use https [default: https].

    --dry-run
        Generate all data, but don't actually perform the final
        xmlimport on the timetable site.

    -X=<name>=<value>
        Extension parameters to send to the data source.
"""

# Data source: - for stdin, or name of generator, e.g. engineering
#    - ttapiutils.autoimport.generators: engineering
# domain to upload to
# list of of paths to be affected
from collections import defaultdict
import functools
import json
import os
import os.path
import re
import sys

import docopt
import pkg_resources

from ttapiutils.canonicalise import canonicalise
from ttapiutils.deletegen import generate_deletes
from ttapiutils.fixexport import fix_export_ids
from ttapiutils.merge import merge
from ttapiutils.utils import (
    DirectoryAuditLogger,
    get_credentials,
    get_proto,
    parse_xml,
    read_password,
    serialise_http_request,
    serialise_http_response,
    TimetableApiUtilsException,
    write_c14n_pretty
)
from ttapiutils.xmlexport import xmlexport
from ttapiutils.xmlimport import xmlimport


class NoSuchDataSourceException(TimetableApiUtilsException):
    pass

class DataSourceParamsException(TimetableApiUtilsException):
    pass


def get_defined_data_source_entrypoints():
    return dict((ep.name, ep)
            for ep in pkg_resources.iter_entry_points(
            group="ttapiutils.autoimport.datasources"))


class StreamDataSource(object):
    def __init__(self, file):
        self.file = file

    @classmethod
    def get_factory_for_file(cls, file):
        return functools.partial(cls.factory, file)

    @classmethod
    def factory(cls, file, params):
        # Ignore audit_log as we have nothing to log
        params.pop("audit_log", None)

        if len(params) != 0:
            raise DataSourceParamsException(
                "StreamDataSource received unexpected params")

        return StreamDataSource(file)

    def get_xml(self):
        return parse_xml(self.file)


def get_data_source_factory(data_source_name, data_source_entrypoints=None):
    if data_source_name == "-":
        return StreamDataSource.get_factory_for_file(sys.stdin)

    if data_source_entrypoints is None:
        data_source_entrypoints = get_defined_data_source_entrypoints()

    if not data_source_name in data_source_entrypoints:
        raise NoSuchDataSourceException("No such data source: {!r}".format(data_source_name))

    entrypoint = data_source_entrypoints[data_source_name]
    return entrypoint.load()


def parse_data_source_args(x_params):
    params = defaultdict(list)
    for item in x_params:
        key, value = item.split("=", 1)
        params[key].append(value)

    # Flatten empty lists
    items = ((key, value[0] if len(value) == 1 else value)
             for (key, value) in params.items())
    return dict(params.items())


class AutoImporter(object):
    def __init__(self, data_source, domain, is_dry_run=False, permitted_paths=None,
                 http_protocol="https", auth=None):
        self.data_source = data_source
        self._is_dry_run = bool(is_dry_run)
        self._permitted_paths = permitted_paths
        self._http_protocol = http_protocol
        self._domain = domain
        self._auth = auth

    def get_paths(self):
        return self._permitted_paths

    def get_proto(self):
        return self._http_protocol

    def get_domain(self):
        return self._domain

    def get_auth(self):
        return self._auth

    def is_dry_run(self):
        return self._is_dry_run

    def get_raw_new_state(self):
        return self.data_source.get_xml()

    def get_canonical_new_state(self):
        return canonicalise(self.get_raw_new_state())

    def get_raw_old_state(self, path):
        return xmlexport(self.get_domain(), path, auth=self.get_auth(),
                         proto=self.get_proto(), fix_ids=False)

    def get_fixed_old_state(self, path):
        """
        As get_raw_old_state() but with event IDs fixed.
        """
        return fix_export_ids(self.get_raw_old_state(path))

    def get_merged_old_state(self):
        return merge(
            self.get_fixed_old_state(path) for path in self.get_paths())

    def get_canonical_merged_old_state(self):
        return canonicalise(self.get_merged_old_state())

    def get_state_with_deletes(self):
        old_state = self.get_canonical_merged_old_state()
        new_state = self.get_canonical_new_state()
        return generate_deletes(old_state, new_state)

    def import_to_timetable(self, api_xml):
        return xmlimport(api_xml, self.get_domain(), paths=self.get_paths(),
                  proto=self.get_proto(), auth=self.get_auth(),
                  dry_run=self.is_dry_run())

    def auto_import(self):
        api_xml = self.get_state_with_deletes()
        self.import_to_timetable(api_xml)


def path_filename_representation(path):
    """
    Get a representation of a timetable path such as /tripos/engineering/IA
    which is sutable for use in a filename.
    """
    # Strip leading / and replace / with .
    return re.sub(r"^/(.*)$", r"\1", path).replace("/", ".")


class AuditTrailAutoImporter(AutoImporter):
    def __init__(self, audit_log, *args, **kwargs):
        super(AuditTrailAutoImporter, self).__init__(*args, **kwargs)
        self._audit_log = audit_log
        self.log_manifest()

    def log_xml(self, name, xml):
        return self._audit_log.log_xml(name, xml)

    def log_manifest(self):
        """Log a JSON file containing the options"""
        self._audit_log.log_json("manifest", self.get_manifest_json())

    def get_manifest_json(self):
        return {
            "pid": os.getpid(),
            "time": self._audit_log.get_timestamp(),
            "permitted_paths": self.get_paths(),
            "http_proto": self.get_proto(),
            "domain": self.get_domain(),
            "is_dry_run": self.is_dry_run()
        }

    # Override XML producing methods to log output to audit dir
    def get_raw_new_state(self):
        return self.log_xml("raw_new_state",
            super(AuditTrailAutoImporter, self).get_raw_new_state())

    def get_canonical_new_state(self):
        return self.log_xml("canonical_new_state",
            super(AuditTrailAutoImporter, self).get_canonical_new_state())

    def get_raw_old_state(self, path):
        return self.log_xml(
            "raw_old_state_{}".format(path_filename_representation(path)),
            super(AuditTrailAutoImporter, self).get_raw_old_state(path))

    def get_fixed_old_state(self, path):
        return self.log_xml(
            "fixed_old_state_{}".format(path_filename_representation(path)),
            super(AuditTrailAutoImporter, self).get_fixed_old_state(path))

    def get_merged_old_state(self):
        return self.log_xml("merged_old_state",
            super(AuditTrailAutoImporter, self).get_merged_old_state())

    def get_canonical_merged_old_state(self):
        return self.log_xml("canonical_merged_old_state",
            super(AuditTrailAutoImporter, self)
            .get_canonical_merged_old_state())

    def get_state_with_deletes(self):
        return self.log_xml("state_with_deletes",
            super(AuditTrailAutoImporter, self).get_state_with_deletes())

    def import_to_timetable(self, api_xml):
        request, response = (super(AuditTrailAutoImporter, self)
            .import_to_timetable(api_xml))

        with self._audit_log.open_audit_file("http_request.txt") as f:
            serialise_http_request(request, f)

        # There's only an HTTP response if it's not a dry run
        if not self.is_dry_run():
            assert response is not None
            with self._audit_log.open_audit_file("http_response.txt") as f:
                serialise_http_response(response, f)


def main(argv):
    args = docopt.docopt(__doc__, argv=argv)

    # TODO: log cmd line args in manifest.json
    credentials = get_credentials(args)
    proto = get_proto(args)
    domain = args["<domain>"]
    paths = args["<path>"]
    audit_trail_base_dir = args["--audit-trail"]
    dry_run = args["--dry-run"]

    data_source_factory = get_data_source_factory(args["<data-source>"])
    data_source_params = parse_data_source_args(args["-X"])

    # Construct an auto importer from our params
    args = [domain]
    kwargs = dict(
        is_dry_run=dry_run, permitted_paths=paths, http_protocol=proto,
        auth=credentials)
    if audit_trail_base_dir is None:
        importer_class = AutoImporter
        data_source = data_source_factory(data_source_params)
        args = [data_source] + args
    else:
        audit_log = DirectoryAuditLogger(audit_trail_base_dir)
        data_source_params["audit_log"] = audit_log
        data_source = data_source_factory(data_source_params)
        importer_class = AuditTrailAutoImporter
        args = [audit_log, data_source] + args

    auto_importer = importer_class(*args, **kwargs)

    # Perform the import
    auto_importer.auto_import()
